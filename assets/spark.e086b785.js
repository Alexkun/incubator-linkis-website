import{_ as n,a as e}from"./workflow.72652f4e.js";import{o as l,c as t,b as a,e as s,r as i,l as u,u as r}from"./vendor.1180558b.js";var o="/assets/sparksql_run.115bb5a7.png",p="/assets/pyspakr-run.9c36d9ef.png",h="/assets/spark-conf.9e59a279.png";const c={class:"markdown-body"},k=[a("h1",null,"Spark engine usage documentation",-1),a("p",null,"This article mainly introduces the configuration, deployment and use of spark engine in Linkis1.0.",-1),a("h2",null,"1. Environment configuration before using Spark engine",-1),a("p",null,"If you want to use the spark engine on your server, you need to ensure that the following environment variables have been set correctly and that the user who started the engine has these environment variables.",-1),a("p",null,"It is strongly recommended that you check these environment variables of the executing user before executing spark tasks.",-1),a("table",null,[a("thead",null,[a("tr",null,[a("th",null,"Environment variable name"),a("th",null,"Environment variable content"),a("th",null,"Remarks")])]),a("tbody",null,[a("tr",null,[a("td",null,"JAVA_HOME"),a("td",null,"JDK installation path"),a("td",null,"Required")]),a("tr",null,[a("td",null,"HADOOP_HOME"),a("td",null,"Hadoop installation path"),a("td",null,"Required")]),a("tr",null,[a("td",null,"HADOOP_CONF_DIR"),a("td",null,"Hadoop configuration path"),a("td",null,"Required")]),a("tr",null,[a("td",null,"HIVE_CONF_DIR"),a("td",null,"Hive configuration path"),a("td",null,"Required")]),a("tr",null,[a("td",null,"SPARK_HOME"),a("td",null,"Spark installation path"),a("td",null,"Required")]),a("tr",null,[a("td",null,"SPARK_CONF_DIR"),a("td",null,"Spark configuration path"),a("td",null,"Required")]),a("tr",null,[a("td",null,"python"),a("td",null,"python"),a("td",null,"Anaconda’s python is recommended as the default python")])])],-1),a("p",null,"Table 1-1 Environmental configuration list",-1),a("h2",null,"2. Configuration and deployment of Spark engine",-1),a("h3",null,"2.1 Selection and compilation of spark version",-1),a("p",null,"In theory, Linkis1.0 supports all versions of spark2.x and above. Spark 2.4.3 is the default supported version. If you want to use your spark version, such as spark2.1.0, you only need to modify the version of the plug-in spark and then compile it. Specifically, you can find the linkis-engineplugin-spark module, change the <spark.version> tag to 2.1.0, and then compile this module separately.",-1),a("h3",null,"2.2 spark engineConn deployment and loading",-1),a("p",null,"If you have already compiled your spark engine plug-in has been compiled, then you need to put the new plug-in to the specified location to load, you can refer to the following article for details",-1),a("p",null,[a("a",{href:"https://github.com/WeBankFinTech/Linkis/wiki/EngineConnPlugin%E5%BC%95%E6%93%8E%E6%8F%92%E4%BB%B6%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3"},"https://github.com/WeBankFinTech/Linkis/wiki/EngineConnPlugin引擎插件安装文档")],-1),a("h3",null,"2.3 tags of spark engine",-1),a("p",null,"Linkis1.0 is done through tags, so we need to insert data in our database, the way of inserting is shown below.",-1),a("p",null,[a("a",{href:"https://github.com/WeBankFinTech/Linkis/wiki/EngineConnPlugin%E5%BC%95%E6%93%8E%E6%8F%92%E4%BB%B6%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3#22-%E7%AE%A1%E7%90%86%E5%8F%B0configuration%E9%85%8D%E7%BD%AE%E4%BF%AE%E6%94%B9%E5%8F%AF%E9%80%89"},"https://github.com/WeBankFinTech/Linkis/wiki/EngineConnPlugin引擎插件安装文档#22-管理台configuration配置修改可选")],-1),a("h2",null,"3. Use of spark engine",-1),a("h3",null,"Preparation for operation, queue setting",-1),a("p",null,"Because the execution of spark is a resource that requires a queue, the user must set up a queue that he can execute before executing.",-1),a("p",null,[a("img",{src:n,alt:""})],-1),a("p",null,"Figure 3-1 Queue settings",-1),a("h3",null,"3.1 How to use Scriptis",-1),a("p",null,"The use of Scriptis is the simplest. You can directly enter Scriptis and create a new sql, scala or pyspark script for execution.",-1),a("p",null,"The sql method is the simplest. You can create a new sql script and write and execute it. When it is executed, the progress will be displayed. If the user does not have a spark engine at the beginning, the execution of sql will start a spark session (it may take some time here), After the SparkSession is initialized, you can start to execute sql.",-1),a("p",null,[a("img",{src:o,alt:""})],-1),a("p",null,"Figure 3-2 Screenshot of the execution effect of sparksql",-1),a("p",null,"For spark-scala tasks, we have initialized sqlContext and other variables, and users can directly use this sqlContext to execute sql.",-1),a("p",null,[a("img",{src:"/assets/scala-run.62f19952.png",alt:""})],-1),a("p",null,"Figure 3-3 Execution effect diagram of spark-scala",-1),a("p",null,"Similarly, in the way of pyspark, we have also initialized the SparkSession, and users can directly use spark.sql to execute SQL.",-1),a("p",null,[a("img",{src:p,alt:""}),s(" Figure 3-4 pyspark execution mode")],-1),a("h3",null,"3.2 How to use workflow",-1),a("p",null,"DSS workflow also has three spark nodes. You can drag in workflow nodes, such as sql, scala or pyspark nodes, and then double-click to enter and edit the code, and then execute in the form of workflow.",-1),a("p",null,[a("img",{src:e,alt:""})],-1),a("p",null,"Figure 3-5 The node where the workflow executes spark",-1),a("h3",null,"3.3 How to use Linkis Client",-1),a("p",null,[s("Linkis also provides a client method to call spark tasks, the call method is through the SDK provided by LinkisClient. We provide java and scala two ways to call, the specific usage can refer to <"),a("a",{href:"https://github.com/WeBankFinTech/Linkis/wiki/Linkis1.0%E7%94%A8%E6%88%B7%E4"},"https://github.com/WeBankFinTech/Linkis/wiki/Linkis1.0用户�"),s(" %BD%BF%E7%94%A8%E6%96%87%E6%A1%A3>.")],-1),a("h2",null,"4. Spark engine user settings",-1),a("p",null,"In addition to the above engine configuration, users can also make custom settings, such as the number of spark session executors and the memory of the executors. These parameters are for users to set their own spark parameters more freely, and other spark parameters can also be modified, such as the python version of pyspark.",-1),a("p",null,[a("img",{src:h,alt:""})],-1),a("p",null,"Figure 4-1 Spark user-defined configuration management console",-1)],d={setup:(n,{expose:e})=>(e({frontmatter:{}}),(n,e)=>(l(),t("div",c,k)))},g={class:"markdown-body"},E=[a("h1",null,"Spark 引擎使用文档",-1),a("p",null,"本文主要介绍在Linkis1.0中，spark引擎的配置、部署和使用。",-1),a("h2",null,"1.Spark引擎使用前的环境配置",-1),a("p",null,"如果您希望在您的服务器上使用spark引擎，您需要保证以下的环境变量已经设置正确并且引擎的启动用户是有这些环境变量的。",-1),a("p",null,"强烈建议您在执行spark任务之前，检查下执行用户的这些环境变量。",-1),a("table",null,[a("thead",null,[a("tr",null,[a("th",null,"环境变量名"),a("th",null,"环境变量内容"),a("th",null,"备注")])]),a("tbody",null,[a("tr",null,[a("td",null,"JAVA_HOME"),a("td",null,"JDK安装路径"),a("td",null,"必须")]),a("tr",null,[a("td",null,"HADOOP_HOME"),a("td",null,"Hadoop安装路径"),a("td",null,"必须")]),a("tr",null,[a("td",null,"HADOOP_CONF_DIR"),a("td",null,"Hadoop配置路径"),a("td",null,"必须")]),a("tr",null,[a("td",null,"HIVE_CONF_DIR"),a("td",null,"Hive配置路径"),a("td",null,"必须")]),a("tr",null,[a("td",null,"SPARK_HOME"),a("td",null,"Spark安装路径"),a("td",null,"必须")]),a("tr",null,[a("td",null,"SPARK_CONF_DIR"),a("td",null,"Spark配置路径"),a("td",null,"必须")]),a("tr",null,[a("td",null,"python"),a("td",null,"python"),a("td",null,"建议使用anaconda的python作为默认python")])])],-1),a("p",null,"表1-1 环境配置清单",-1),a("h2",null,"2.Spark引擎的配置和部署",-1),a("h3",null,"2.1 spark版本的选择和编译",-1),a("p",null,"理论上Linkis1.0支持的spark2.x以上的所有版本。默认支持的版本Spark2.4.3。如果您想使用您的spark版本，如spark2.1.0，则您仅仅需要将插件spark的版本进行修改，然后进行编译即可。具体的，您可以找到linkis-engineplugin-spark模块，将<spark.version>标签进行改成2.1.0，然后单独编译此模块即可。",-1),a("h3",null,"2.2 spark engineConn部署和加载",-1),a("p",null,"如果您已经编译完了您的spark引擎的插件已经编译完成，那么您需要将新的插件放置到指定的位置中才能加载，具体可以参考下面这篇文章",-1),a("p",null,[a("a",{href:"https://github.com/WeBankFinTech/Linkis/wiki/EngineConnPlugin%E5%BC%95%E6%93%8E%E6%8F%92%E4%BB%B6%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3"},"https://github.com/WeBankFinTech/Linkis/wiki/EngineConnPlugin引擎插件安装文档")],-1),a("h3",null,"2.3 spark引擎的标签",-1),a("p",null,"Linkis1.0是通过标签来进行的，所以需要在我们数据库中插入数据，插入的方式如下文所示。",-1),a("p",null,[a("a",{href:"https://github.com/WeBankFinTech/Linkis/wiki/EngineConnPlugin%E5%BC%95%E6%93%8E%E6%8F%92%E4%BB%B6%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3#22-%E7%AE%A1%E7%90%86%E5%8F%B0configuration%E9%85%8D%E7%BD%AE%E4%BF%AE%E6%94%B9%E5%8F%AF%E9%80%89"},"https://github.com/WeBankFinTech/Linkis/wiki/EngineConnPlugin引擎插件安装文档#22-管理台configuration配置修改可选")],-1),a("h2",null,"3.spark引擎的使用",-1),a("h3",null,"准备操作，队列设置",-1),a("p",null,"因为spark的执行是需要队列的资源，所以用户在执行之前，必须要设置自己能够执行的队列。",-1),a("p",null,[a("img",{src:n,alt:""})],-1),a("p",null,"图3-1 队列设置",-1),a("h3",null,"3.1 Scriptis的使用方式",-1),a("p",null,"Scriptis的使用方式是最简单的，您可以直接进入Scriptis，新建sql、scala或者pyspark脚本进行执行。",-1),a("p",null,"sql的方式是最简单的，您可以新建sql脚本然后编写进行执行，执行的时候，会有进度的显示。如果一开始用户是没有spark引擎的话，sql的执行会启动一个spark会话(这里可能会花一些时间)， SparkSession初始化之后，就可以开始执行sql。",-1),a("p",null,[a("img",{src:o,alt:""})],-1),a("p",null,"图3-2 sparksql的执行效果截图",-1),a("p",null,"spark-scala的任务，我们已经初始化好了sqlContext等变量，用户可以直接使用这个sqlContext进行sql的执行。",-1),a("p",null,[a("img",{src:"/assets/scala-run.62f19952.png",alt:""})],-1),a("p",null,"图3-3 spark-scala的执行效果图",-1),a("p",null,"类似的，pyspark的方式中，我们也已经初始化好了SparkSession，用户可以直接使用spark.sql的方式进行执行sql。",-1),a("p",null,[a("img",{src:p,alt:""}),s(" 图3-4 pyspark的执行方式")],-1),a("h3",null,"3.2工作流的使用方式",-1),a("p",null,"DSS工作流也是有spark的三个节点，您可以拖入工作流节点，如sql、scala或者pyspark节点，然后双击进入然后进行编辑代码，然后以工作流的形式进行执行。",-1),a("p",null,[a("img",{src:e,alt:""})],-1),a("p",null,"图3-5 工作流执行spark的节点",-1),a("h3",null,"3.3 Linkis Client的使用方式",-1),a("p",null,[s("Linkis也提供了client的方式进行调用spark的任务，调用的方式是通过LinkisClient提供的SDK的方式。我们提供了java和scala两种方式进行调用，具体的使用方式可以参考"),a("a",{href:"https://github.com/WeBankFinTech/Linkis/wiki/Linkis1.0%E7%94%A8%E6%88%B7%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3"},"https://github.com/WeBankFinTech/Linkis/wiki/Linkis1.0用户使用文档"),s("。")],-1),a("h2",null,"4.spark引擎的用户设置",-1),a("p",null,"除了以上引擎配置，用户还可以进行自定义的设置，比如spark会话executor个数和executor的内存。这些参数是为了用户能够更加自由地设置自己的spark的参数，另外spark其他参数也可以进行修改，比如的pyspark的python版本等。",-1),a("p",null,[a("img",{src:h,alt:""})],-1),a("p",null,"图4-1 spark的用户自定义配置管理台",-1)],f={setup:(n,{expose:e})=>(e({frontmatter:{}}),(n,e)=>(l(),t("div",g,E)))},m={setup(n){const e=i(localStorage.getItem("locale")||"en");return(n,t)=>"en"===e.value?(l(),u(r(d),{key:0})):(l(),u(r(f),{key:1}))}};export{m as default};
