import{o as e,c as a,b as n,e as o,r,l as t,u as i}from"./vendor.1180558b.js";var l="/assets/linkis-exception-01.a30b0cae.png",s="/assets/linkis-exception-02.c5d295a9.png",c="/assets/hive-config-01.e5d22d71.png",p="/assets/linkis-exception-03.8fc2f10f.png",u="/assets/page-show-01.f6ac5799.png",h="/assets/db-config-01.5aa0a782.png",d="/assets/linkis-exception-04.bb6736c1.png",g="/assets/shell-error-01.2e9d62b8.png",v="/assets/linkis-exception-05.9b7af564.png",m="/assets/page-show-02.9d59cdcb.png",k="/assets/linkis-exception-06.ecfa4a11.png",x="/assets/linkis-exception-07.a1f28559.png",$="/assets/linkis-exception-08.dcdf1ce1.png",f="/assets/shell-error-02.fba39b7b.png",j="/assets/linkis-exception-09.f06ff470.png",y="/assets/linkis-exception-10.49a3d1ba.png",b="/assets/shell-error-03.666f92e3.png",C="/assets/shell-error-04.910b89a7.png",w="/assets/shell-error-05.f4057bcc.png";const E={class:"markdown-body"},F=[n("p",null,[o("Linkis1.0 common problems and solutions :"),n("a",{href:"https://docs.qq.com/doc/DWlN4emlJeEJxWlR0"},"https://docs.qq.com/doc/DWlN4emlJeEJxWlR0")],-1),n("h4",null,"Q1, linkis startup error: NoSuchMethodErrorgetSessionManager()Lorg/eclipse/jetty/server/SessionManager",-1),n("p",null,"Specific stack:",-1),n("pre",null,[n("code",null,"Failed startup of context osbwejJettyEmbeddedWebAppContext@6c6919ff{application,/,[file:///tmp/jetty-docbase.9102.6375358926927953589/],UNAVAILABLE} java.lang.NoSuchMethodError: org.eclipse.jetty.server.session.SessionHandler.getSessionManager ()Lorg/eclipse/jetty/server/SessionManager;\nat org.eclipse.jetty.servlet.ServletContextHandler\\$Context.getSessionCookieConfig(ServletContextHandler.java:1415) ~[jetty-servlet-9.3.20.v20170531.jar:9.3.20.v20170531]\n")],-1),n("p",null,"Solution: jetty-servlet and jetty-security versions need to be upgraded from 9.3.20 to 9.4.20;",-1),n("h4",null,"Q2. When starting the microservice linkis-ps-cs, report DebuggClassWriter overrides final method visit",-1),n("p",null,"Specific exception stack:",-1),n("p",null,[n("img",{src:l,alt:"linkis-exception-01.png"})],-1),n("p",null,"Solution: jar package conflict, delete asm-5.0.4.jar;",-1),n("h4",null,"Q3. When starting the microservice linkis-ps-datasource, JdbcUtils.getDriverClassName NPE",-1),n("p",null,"Specific exception stack:",-1),n("p",null,[n("img",{src:s,alt:"linkis-exception-02.png"})],-1),n("p",null,"Solution: caused by the Linkis-datasource configuration problem, modify the three parameters at the beginning of linkis.properties hive.meta:",-1),n("p",null,[n("img",{src:c,alt:"hive-config-01.png"})],-1),n("h4",null,"Q4. When starting the microservice linkis-ps-datasource, the following exception ClassNotFoundException HttpClient is reported:",-1),n("p",null,"Specific exception stack:",-1),n("p",null,[n("img",{src:p,alt:"linkis-exception-03.png"})],-1),n("p",null,"Solution: There is a problem with linkis-metadata-dev-1.0.0.jar compiled in 1.0, and it needs to be recompiled and packaged.",-1),n("h4",null,"Q5. Click scriptis-database, no data is returned, the phenomenon is as follows:",-1),n("p",null,[n("img",{src:u,alt:"page-show-01.png"})],-1),n("p",null,"Solution: The reason is that hive is not authorized to Hadoop users. The authorization data is as follows:",-1),n("p",null,[n("img",{src:h,alt:"db-config-01.png"})],-1),n("h4",null,"Q6, shell engine scheduling execution, the page reports Insufficient resource, requesting available engine timeout, eneningeconnmanager linkis.out, and the following error is reported:",-1),n("p",null,[n("img",{src:d,alt:"linkis-exception-04.png"})],-1),n("p",null,"Solution: The reason Hadoop did not create /appcom/tmp/hadoop/workDir. Create it in advance through the root user, and then authorize the Hadoop user.",-1),n("h4",null,"Q7. When the shell engine is scheduled for execution, the engine execution directory reports the following error /bin/java: No such file or directory:",-1),n("p",null,[n("img",{src:g,alt:"shell-error-01.png"})],-1),n("p",null,"Solution: There is a problem with the local java environment variables, and you need to make a symbolic link to the java command.",-1),n("h4",null,"Q8, hive engine scheduling, the following error is reported EngineConnPluginNotFoundException:errorCode:70063",-1),n("p",null,[n("img",{src:v,alt:"linkis-exception-05.png"})],-1),n("p",null,"Solution: It is caused by not modifying the version of the corresponding engine during installation, so the engine type inserted into the db by default is the default version, and the compiled version is not caused by the default version. Specific modification steps: cd /appcom/Install/dss-linkis/linkis/lib/linkis-engineconn-plugins/, modify the v2.1.1 directory name in the dist directory to v1.2.1 modify the subdirectory name in the plugin directory 2.1. 1 is 1.2.1 of the default version. If it is Spark, you need to modify dist/v2.4.3 and plugin/2.4.3 accordingly. Finally restart the engineplugin service.",-1),n("h4",null,"Q9. After the linkis microservice is started, the following error is reported: Load balancer does not have available server for client:",-1),n("p",null,[n("img",{src:m,alt:"page-show-02.png"})],-1),n("p",null,"Solution: This is because the linkis microservice has just started and the registration has not been completed. Wait for 1~2 minutes and try again.",-1),n("h4",null,"Q10. When the hive engine is scheduled for execution, the following error is reported: operation failed NullPointerException:",-1),n("p",null,[n("img",{src:k,alt:"linkis-exception-06.png"})],-1),n("p",null,"Solution: The server lacks environment variables, add export HIVE_CONF_DIR=/etc/hive/conf in /etc/profile;",-1),n("h4",null,"Q11. When hive engine is scheduled, the error log of engineConnManager is as follows method did not exist: SessionHandler:",-1),n("p",null,[n("img",{src:x,alt:"linkis-exception-07.png"})],-1),n("p",null,"Solution: Under the hive engine lib, the jetty jar package conflicts, replace jetty-security and jetty-server with 9.4.20;",-1),n("h4",null,"After Q12, hive engine restarts, the jar package of jetty 9.4 is always replaced by 9.3",-1),n("p",null,"Solution: When the engine instance is generated, there will be a jar package cache. First, you need to delete the records related to the table linkis_engine_conn_plugin_bml_resources hive, and then delete the records under the directory /appcom/Install/dss-linkis/linkis/lib/linkis-engineconn-plugins/hive/dist 1.2.1.zip, finally restart the engineplugin service, the jar package of lib will be updated successfully.",-1),n("h4",null,"Q13. When the hive engine is executed, the following error is reported: Lcom/google/common/collect/UnmodifiableIterator:",-1),n("pre",null,[n("code",null,"2021-03-16 13:32:23.304 ERROR [pool-2-thread-1] com.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor 140 run-query failed, reason: java.lang.IllegalAccessError: tried to access method com.google.common.collect.Iterators.emptyIterator() Lcom/google/common/collect/UnmodifiableIterator; from class org.apache.hadoop.hive.ql.exec.FetchOperator\nat org.apache.hadoop.hive.ql.exec.FetchOperator.<init>(FetchOperator.java:108) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.exec.FetchTask.initialize(FetchTask.java:86) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:629) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1414) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1543) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1332) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1321) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat com.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor$$anon$1.run(HiveEngineConnExecutor.scala:152) [linkis-engineplugin-hive-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor$$anon$1.run(HiveEngineConnExecutor.scala:126) [linkis-engineplugin-hive-dev-1.0.0.jar:?]\n")],-1),n("p",null,"Solution: guava package conflict, kill guava-25.1-jre.jar under hive/dist/v1.2.1/lib;",-1),n("h4",null,"Q14. When the hive engine is executed, the error is reported as follows: TaskExecutionServiceImpl 59 error-org/apache/curator/connection/ConnectionHandlingPolicy:",-1),n("pre",null,[n("code",null,"2021-03-16 16:17:40.649 INFO [pool-2-thread-1] com.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor 42 info-com.webank.wedatasphere.linkis.engineplugin.hive. executor.HiveEngineConnExecutor@36a7c96f change status Busy => Idle.\n2021-03-16 16:17:40.661 ERROR [pool-2-thread-1] com.webank.wedatasphere.linkis.engineconn.computation.executor.service.TaskExecutionServiceImpl 59 error-org/apache/curator/connection/ConnectionHandlingPolicy java .lang.NoClassDefFoundError: org/apache/curator/connection/ConnectionHandlingPolicy at org.apache.curator.framework.CuratorFrameworkFactory.builder(CuratorFrameworkFactory.java:78) ~[curator-framework-4.0.1.jar:4.0.1]\nat org.apache.hadoop.hive.ql.lockmgr.zookeeper.CuratorFrameworkSingleton.getInstance(CuratorFrameworkSingleton.java:59) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.setContext(ZooKeeperHiveLockManager.java:98) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.getLockManager(DummyTxnManager.java:87) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.acquireLocks(DummyTxnManager.java:121) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.acquireLocksAndOpenTxn(Driver.java:1237) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1607) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1332) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1321) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat com.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor$$anon$1.run(HiveEngineConnExecutor.scala:152) ~[linkis-engineplugin-hive-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor$$anon$1.run(HiveEngineConnExecutor.scala:126) ~[linkis-engineplugin-hive-dev-1.0.0.jar:?]\nat java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_181]\nat javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_181]\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875) ~[hadoop-common-3.0.0-cdh6.3.2.jar:?]\nat com.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor.executeLine(HiveEngineConnExecutor.scala:126) ~[linkis-engineplugin-hive-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.execute.ComputationExecutor$$anonfun$execute$1$$anonfun$apply$9$$anonfun$apply$10.apply(ComputationExecutor.scala:145) ~[linkis-computation -engineconn-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.execute.ComputationExecutor$$anonfun$execute$1$$anonfun$apply$9$$anonfun$apply$10.apply(ComputationExecutor.scala:144) ~[linkis-computation -engineconn-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.common.utils.Utils$.tryCatch(Utils.scala:48) ~[linkis-common-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.execute.ComputationExecutor$$anonfun$execute$1$$anonfun$apply$9.apply(ComputationExecutor.scala:146) ~[linkis-computation-engineconn-dev-1.0 .0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.execute.ComputationExecutor$$anonfun$execute$1$$anonfun$apply$9.apply(ComputationExecutor.scala:140) ~[linkis-computation-engineconn-dev-1.0 .0.jar:?]\nat scala.collection.immutable.Range.foreach(Range.scala:160) ~[scala-library-2.11.8.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.execute.ComputationExecutor$$anonfun$execute$1.apply(ComputationExecutor.scala:139) ~[linkis-computation-engineconn-dev-1.0.0.jar:? ]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.execute.ComputationExecutor$$anonfun$execute$1.apply(ComputationExecutor.scala:114) ~[linkis-computation-engineconn-dev-1.0.0.jar:? ]\nat com.webank.wedatasphere.linkis.common.utils.Utils$.tryFinally(Utils.scala:62) ~[linkis-common-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.acessible.executor.entity.AccessibleExecutor.ensureIdle(AccessibleExecutor.scala:42) ~[linkis-accessible-executor-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.acessible.executor.entity.AccessibleExecutor.ensureIdle(AccessibleExecutor.scala:36) ~[linkis-accessible-executor-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.execute.ComputationExecutor.ensureOp(ComputationExecutor.scala:103) ~[linkis-computation-engineconn-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.execute.ComputationExecutor.execute(ComputationExecutor.scala:114) ~[linkis-computation-engineconn-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.service.TaskExecutionServiceImpl$$anon$1$$anonfun$run$1.apply$mcV$sp(TaskExecutionServiceImpl.scala:139) [linkis-computation-engineconn-dev- 1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.service.TaskExecutionServiceImpl$$anon$1$$anonfun$run$1.apply(TaskExecutionServiceImpl.scala:138) [linkis-computation-engineconn-dev-1.0.0. jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.service.TaskExecutionServiceImpl$$anon$1$$anonfun$run$1.apply(TaskExecutionServiceImpl.scala:138) [linkis-computation-engineconn-dev-1.0.0. jar:?]\nat com.webank.wedatasphere.linkis.common.utils.Utils$.tryCatch(Utils.scala:48) [linkis-common-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.common.utils.Utils$.tryAndWarn(Utils.scala:74) [linkis-common-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.service.TaskExecutionServiceImpl$$anon$1.run(TaskExecutionServiceImpl.scala:138) [linkis-computation-engineconn-dev-1.0.0.jar:?]\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_181]\nat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_181]\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_181]\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_181]\nat java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]\nCaused by: java.lang.ClassNotFoundException: org.apache.curator.connection.ConnectionHandlingPolicy atjava.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[?:1.8.0_181]\nat java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_181]\nat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) ~[?:1.8.0_181]\nat java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_181]\n... 39 more\n")],-1),n("p",null,[o("Solution: The reason is that there is a corresponding relationship between the version of Curator and the version of zookeeper. For Curator2.X, it supports Zookeeper3.4.X for Curator2.X, so if you are currently Zookeeper3.4.X, you should still use Curator2.X, for example: 2.7.0. Reference link: "),n("a",{href:"https://blog.csdn.net/muyingmiao/article/details/100183768"},"https://blog.csdn.net/muyingmiao/article/details/100183768")],-1),n("h4",null,"Q15. When the python engine is scheduled, the following error is reported: Python proces is not alive:",-1),n("p",null,[n("img",{src:$,alt:"linkis-exception-08.png"})],-1),n("p",null,"Solution: The server installed the anaconda3 package manager. After debugging python, two problems were found: (1) lack of pandas and matplotlib modules, which need to be installed manually; (2) when the new version of the python engine is executed, it depends on the higher version of python, first install python3, Next, make a symbolic link (as shown in the figure below) and restart the engineplugin service.",-1),n("p",null,[n("img",{src:f,alt:"shell-error-02.png"})],-1),n("h4",null,"Q16. When the spark engine is executed, the following error NoClassDefFoundError: org/apache/hadoop/hive/ql/io/orc/OrcFile is reported:",-1),n("pre",null,[n("code",null,"2021-03-19 15:12:49.227 INFO [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler 57 logInfo -ShuffleMapStage 5 (show at <console>:69) failed in 21.269 s due to Job aborted due to stage failure: Task 1 in stage 5.0 failed 4 times, most recent failure: Lost task 1.3 in stage 5.0 (TID 139, cdh03, executor 6): java.lang.NoClassDefFoundError: org/apache/hadoop/hive/ql /io/orc/OrcFile\nat org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2.apply(OrcFileOperator.scala:75)\nat org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2.apply(OrcFileOperator.scala:73)\nat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\nat scala.collection.TraversableOnce$class.collectFirst(TraversableOnce.scala:145)\nat scala.collection.AbstractIterator.collectFirst(Iterator.scala:1334)\nat org.apache.spark.sql.hive.orc.OrcFileOperator$.getFileReader(OrcFileOperator.scala:90)\nat org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$2.apply(OrcFileOperator.scala:99)\nat org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$2.apply(OrcFileOperator.scala:99)\nat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\nat scala.collection.TraversableOnce$class.collectFirst(TraversableOnce.scala:145)\nat scala.collection.AbstractIterator.collectFirst(Iterator.scala:1334)\nat org.apache.spark.sql.hive.orc.OrcFileOperator$.readSchema(OrcFileOperator.scala:99)\nat org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2.apply(OrcFileFormat.scala:160)\nat org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2.apply(OrcFileFormat.scala:151)\nat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\nat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\nat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:126)\nat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:179)\nat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:103)\nat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(UnknownSource)\nat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:624)\nat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\nat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\nat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\nat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\nat org.apache.spark.scheduler.Task.run(Task.scala:121)\nat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\nat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\nat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\nat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.io.orc.OrcFile\nat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\nat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n... 33 more\n\n")],-1),n("p",null,"Solution: cdh6.3.2 cluster spark engine classpath only has /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/jars, need to add hive-exec-2.1.1- cdh6.1.0.jar, then restart spark.",-1),n("h4",null,"Q17. When the spark engine starts, it reports queue default is not exists in YARN, the specific information is as follows:",-1),n("p",null,[n("img",{src:j,alt:"linkis-exception-09.png"})],-1),n("p",null,"Solution: When the 1.0 linkis-resource-manager-dev-1.0.0.jar pulls queue information, there is a compatibility problem in parsing json. After the official classmates optimize it, re-provide a new package. The jar package path: /appcom/Install/dss- linkis/linkis/lib/linkis-computation-governance/linkis-cg-linkismanager/.",-1),n("h4",null,"Q18, when the spark engine starts, an error is reported get the Yarn queue information excepiton. (get the Yarn queue information abnormal) and http link abnormal",-1),n("p",null,"Solution: To migrate the address configuration of yarn to the DB configuration, the following configuration needs to be added:",-1),n("p",null,[n("img",{src:"/assets/db-config-02.f05b1586.png",alt:"db-config-02.png"})],-1),n("h4",null,"Q19. When the spark engine is scheduled, it can be executed successfully for the first time, and if executed again, it will report Spark application sc has already stopped, please restart it. The specific errors are as follows:",-1),n("p",null,[n("img",{src:"/assets/page-show-03.63498698.png",alt:"page-show-03.png"})],-1),n("p",null,"Solution: The background is that the architecture of the linkis1.0 engine has been adjusted. After the spark session is created, in order to avoid overhead and improve execution efficiency, the session is reused. When we execute spark.scala for the first time, there is spark.stop() in our script. This command will cause the newly created session to be closed. When executed again, it will prompt that the session is closed, please restart it. Solution: first remove stop() from all scripts, and then execute the order: execute default.sql first, then execute scalaspark and pythonspark.",-1),n("h4",null,"Q20, pythonspark scheduling execution, error: initialize python executor failed ClassNotFoundException org.slf4j.impl.StaticLoggerBinder, as follows:",-1),n("p",null,[n("img",{src:y,alt:"linkis-exception-10.png"})],-1),n("p",null,"Solution: The reason is that the spark server lacks slf4j-log4j12-1.7.25.jar, copy the above jar and report to /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/jars .",-1),n("h4",null,"Q21, pythonspark scheduling execution, error: initialize python executor failed, submit-version error, as follows:",-1),n("p",null,[n("img",{src:b,alt:"shell-error-03.png"})],-1),n("p",null,"Solution: The reason is that the linkis1.0 pythonSpark engine has a bug in obtaining the spark version code. The fix is ​​as follows:",-1),n("p",null,[n("img",{src:"/assets/code-fix-01.620f0486.png",alt:"code-fix-01.png"})],-1),n("h4",null,"Q22. When pythonspark is scheduled to execute, it reports TypeError: an integer is required (got type bytes) (executed separately from the command to pull up the engine), the details are as follows:",-1),n("p",null,[n("img",{src:C,alt:"shell-error-04.png"})],-1),n("p",null,"Solution: The reason is that the system spark and python versions are not compatible, python is 3.8, spark is 2.4.0-cdh6.3.2, spark requires python version<=3.6, reduce python to 3.6, comment file /opt/cloudera/parcels/CDH/ The following lines of lib/spark/python/lib/pyspark.zip/pyspark/context.py:",-1),n("p",null,[n("img",{src:w,alt:"shell-error-05.png"})],-1),n("h4",null,"Q23, spark engine is 2.4.0+cdh6.3.2, python engine was previously lacking pandas, matplotlib upgraded local python to 3.8, but spark does not support python3.8, only supports below 3.6;",-1),n("p",null,"Solution: reinstall the python package manager anaconda2, reduce python to 2.7, install pandas, matplotlib modules, python engine and spark engine can be scheduled normally.",-1)],S={setup:(n,{expose:o})=>(o({frontmatter:{}}),(n,o)=>(e(),a("div",E,F)))},T={class:"markdown-body"},D=[n("p",null,[o("Linkis1.0常见问题和解决办法 ："),n("a",{href:"https://docs.qq.com/doc/DWlN4emlJeEJxWlR0"},"https://docs.qq.com/doc/DWlN4emlJeEJxWlR0")],-1),n("h4",null,"Q1、linkis启动报错：NoSuchMethodErrorgetSessionManager()Lorg/eclipse/jetty/server/SessionManager",-1),n("p",null,"具体堆栈：",-1),n("pre",null,[n("code",null,"Failed startup of context o.s.b.w.e.j.JettyEmbeddedWebAppContext@6c6919ff{application,/,[file:///tmp/jetty-docbase.9102.6375358926927953589/],UNAVAILABLE} java.lang.NoSuchMethodError: org.eclipse.jetty.server.session.SessionHandler.getSessionManager()Lorg/eclipse/jetty/server/SessionManager;\nat org.eclipse.jetty.servlet.ServletContextHandler\\$Context.getSessionCookieConfig(ServletContextHandler.java:1415) ~[jetty-servlet-9.3.20.v20170531.jar:9.3.20.v20170531]\n")],-1),n("p",null,"解法：jetty-servlet 和 jetty-security版本需要从9.3.20升级为9.4.20；",-1),n("h4",null,"Q2、启动微服务linkis-ps-cs时，报DebuggClassWriter overrides final method visit",-1),n("p",null,"具体异常栈：",-1),n("p",null,[n("img",{src:l,alt:"linkis-exception-01.png"})],-1),n("p",null,"解法:jar包冲突，删除asm-5.0.4.jar;",-1),n("h4",null,"Q3、启动微服务linkis-ps-datasource时，JdbcUtils.getDriverClassName NPE",-1),n("p",null,"具体异常栈：",-1),n("p",null,[n("img",{src:s,alt:"linkis-exception-02.png"})],-1),n("p",null,"解法：Linkis-datasource 配置问题导致的，修改linkis.properties hive.meta开头的三个参数：",-1),n("p",null,[n("img",{src:c,alt:"hive-config-01.png"})],-1),n("h4",null,"Q4、启动微服务linkis-ps-datasource时，报如下异常ClassNotFoundException HttpClient：",-1),n("p",null,"具体异常栈：",-1),n("p",null,[n("img",{src:p,alt:"linkis-exception-03.png"})],-1),n("p",null,"解法：1.0编译的linkis-metadata-dev-1.0.0.jar存在问题，需要重新编译打包。",-1),n("h4",null,"Q5、点击scriptis-数据库，不返回数据，现象如下：",-1),n("p",null,[n("img",{src:u,alt:"page-show-01.png"})],-1),n("p",null,"解法：原因hive未授权给hadoop用户，授权数据如下：",-1),n("p",null,[n("img",{src:h,alt:"db-config-01.png"})],-1),n("h4",null,"Q6、shell引擎调度执行,页面报 Insufficient resource , requesting available engine timeout，eningeconnmanager的linkis.out，报如下错误：",-1),n("p",null,[n("img",{src:d,alt:"linkis-exception-04.png"})],-1),n("p",null,"解法：原因hadoop没有创建/appcom/tmp/hadoop/workDir,通过root用户提前创建，然后给hadoop用户授权即可。",-1),n("h4",null,"Q7、shell引擎调度执行时，引擎执行目录报如下错误/bin/java:No such file or directory：",-1),n("p",null,[n("img",{src:g,alt:"shell-error-01.png"})],-1),n("p",null,"解法：本地java的环境变量有问题，需要对java命令做下符号链接。",-1),n("h4",null,"Q8、hive引擎调度时，报如下错误EngineConnPluginNotFoundException:errorCode:70063",-1),n("p",null,[n("img",{src:v,alt:"linkis-exception-05.png"})],-1),n("p",null,"解法：安装的时候没有修改对应引擎的Version导致，所以默认插入到db里面的引擎类型为默认版本，而编译出来的版本不是默认版本导致。具体修改步骤：cd /appcom/Install/dss-linkis/linkis/lib/linkis-engineconn-plugins/，修改dist目录下的v2.1.1 目录名 修改为v1.2.1 修改plugin目录下的子目录名2.1.1 为默认版本的1.2.1。如果是Spark需要相应修改dist/v2.4.3 和plugin/2.4.3。最后重启engineplugin服务。",-1),n("h4",null,"Q9、linkis微服务启动后，报如下错误Load balancer does not have available server for client：",-1),n("p",null,[n("img",{src:m,alt:"page-show-02.png"})],-1),n("p",null,"解法：这个是因为linkis微服务刚启动，还未完成注册，等待1~2分钟，重试即可。",-1),n("h4",null,"Q10、hive引擎调度执行时，报错如下opertion failed NullPointerException：",-1),n("p",null,[n("img",{src:k,alt:"linkis-exception-06.png"})],-1),n("p",null,"解法：服务器缺少环境变量，/etc/profile增加export HIVE_CONF_DIR=/etc/hive/conf;",-1),n("h4",null,"Q11、hive引擎调度时，engineConnManager的错误日志如下method did not exist:SessionHandler：",-1),n("p",null,[n("img",{src:x,alt:"linkis-exception-07.png"})],-1),n("p",null,"解法：hive引擎lib下，jetty jar包冲突，jetty-security、 jetty-server替换为9.4.20；",-1),n("h4",null,"Q12、hive引擎重启后，jetty 9.4的jar包总是被9.3的替换",-1),n("p",null,"解法：生成引擎实例时，会有jar包缓存，首先需要删除表linkis_engine_conn_plugin_bml_resources hive相关的记录，其次删除目录/appcom/Install/dss-linkis/linkis/lib/linkis-engineconn-plugins/hive/dist下的1.2.1.zip，最后重启engineplugin服务，lib的jar包才会更新成功。",-1),n("h4",null,"Q13、hive引擎执行时，报如下错误Lcom/google/common/collect/UnmodifiableIterator：",-1),n("pre",null,[n("code",null,"2021-03-16 13:32:23.304 ERROR [pool-2-thread-1] com.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor 140 run - query failed, reason : java.lang.IllegalAccessError: tried to access method com.google.common.collect.Iterators.emptyIterator() Lcom/google/common/collect/UnmodifiableIterator; from class org.apache.hadoop.hive.ql.exec.FetchOperator \nat org.apache.hadoop.hive.ql.exec.FetchOperator.<init>(FetchOperator.java:108) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.exec.FetchTask.initialize(FetchTask.java:86) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:629) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1414) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1543) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1332) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1321) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat com.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor$$anon$1.run(HiveEngineConnExecutor.scala:152) [linkis-engineplugin-hive-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor$$anon$1.run(HiveEngineConnExecutor.scala:126) [linkis-engineplugin-hive-dev-1.0.0.jar:?]\n")],-1),n("p",null,"解法：guava包冲突，干掉hive/dist/v1.2.1/lib下的guava-25.1-jre.jar；",-1),n("h4",null,"Q14、hive引擎执行时，报错误如下TaskExecutionServiceImpl 59 error - org/apache/curator/connection/ConnectionHandlingPolicy ：",-1),n("pre",null,[n("code",null,"2021-03-16 16:17:40.649 INFO  [pool-2-thread-1] com.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor 42 info - com.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor@36a7c96f change status Busy => Idle.\n2021-03-16 16:17:40.661 ERROR [pool-2-thread-1] com.webank.wedatasphere.linkis.engineconn.computation.executor.service.TaskExecutionServiceImpl 59 error - org/apache/curator/connection/ConnectionHandlingPolicy java.lang.NoClassDefFoundError: org/apache/curator/connection/ConnectionHandlingPolicy at org.apache.curator.framework.CuratorFrameworkFactory.builder(CuratorFrameworkFactory.java:78) ~[curator-framework-4.0.1.jar:4.0.1]\nat org.apache.hadoop.hive.ql.lockmgr.zookeeper.CuratorFrameworkSingleton.getInstance(CuratorFrameworkSingleton.java:59) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.setContext(ZooKeeperHiveLockManager.java:98) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.getLockManager(DummyTxnManager.java:87) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.acquireLocks(DummyTxnManager.java:121) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.acquireLocksAndOpenTxn(Driver.java:1237) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1607) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1332) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1321) ~[hive-exec-2.1.1-cdh6.1.0.jar:2.1.1-cdh6.1.0]\nat com.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor$$anon$1.run(HiveEngineConnExecutor.scala:152) ~[linkis-engineplugin-hive-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor$$anon$1.run(HiveEngineConnExecutor.scala:126) ~[linkis-engineplugin-hive-dev-1.0.0.jar:?]\nat java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_181]\nat javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_181]\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875) ~[hadoop-common-3.0.0-cdh6.3.2.jar:?]\nat com.webank.wedatasphere.linkis.engineplugin.hive.executor.HiveEngineConnExecutor.executeLine(HiveEngineConnExecutor.scala:126) ~[linkis-engineplugin-hive-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.execute.ComputationExecutor$$anonfun$execute$1$$anonfun$apply$9$$anonfun$apply$10.apply(ComputationExecutor.scala:145) ~[linkis-computation-engineconn-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.execute.ComputationExecutor$$anonfun$execute$1$$anonfun$apply$9$$anonfun$apply$10.apply(ComputationExecutor.scala:144) ~[linkis-computation-engineconn-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.common.utils.Utils$.tryCatch(Utils.scala:48) ~[linkis-common-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.execute.ComputationExecutor$$anonfun$execute$1$$anonfun$apply$9.apply(ComputationExecutor.scala:146) ~[linkis-computation-engineconn-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.execute.ComputationExecutor$$anonfun$execute$1$$anonfun$apply$9.apply(ComputationExecutor.scala:140) ~[linkis-computation-engineconn-dev-1.0.0.jar:?]\nat scala.collection.immutable.Range.foreach(Range.scala:160) ~[scala-library-2.11.8.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.execute.ComputationExecutor$$anonfun$execute$1.apply(ComputationExecutor.scala:139) ~[linkis-computation-engineconn-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.execute.ComputationExecutor$$anonfun$execute$1.apply(ComputationExecutor.scala:114) ~[linkis-computation-engineconn-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.common.utils.Utils$.tryFinally(Utils.scala:62) ~[linkis-common-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.acessible.executor.entity.AccessibleExecutor.ensureIdle(AccessibleExecutor.scala:42) ~[linkis-accessible-executor-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.acessible.executor.entity.AccessibleExecutor.ensureIdle(AccessibleExecutor.scala:36) ~[linkis-accessible-executor-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.execute.ComputationExecutor.ensureOp(ComputationExecutor.scala:103) ~[linkis-computation-engineconn-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.execute.ComputationExecutor.execute(ComputationExecutor.scala:114) ~[linkis-computation-engineconn-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.service.TaskExecutionServiceImpl$$anon$1$$anonfun$run$1.apply$mcV$sp(TaskExecutionServiceImpl.scala:139) [linkis-computation-engineconn-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.service.TaskExecutionServiceImpl$$anon$1$$anonfun$run$1.apply(TaskExecutionServiceImpl.scala:138) [linkis-computation-engineconn-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.service.TaskExecutionServiceImpl$$anon$1$$anonfun$run$1.apply(TaskExecutionServiceImpl.scala:138) [linkis-computation-engineconn-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.common.utils.Utils$.tryCatch(Utils.scala:48) [linkis-common-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.common.utils.Utils$.tryAndWarn(Utils.scala:74) [linkis-common-dev-1.0.0.jar:?]\nat com.webank.wedatasphere.linkis.engineconn.computation.executor.service.TaskExecutionServiceImpl$$anon$1.run(TaskExecutionServiceImpl.scala:138) [linkis-computation-engineconn-dev-1.0.0.jar:?]\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_181]\nat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_181]\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_181]\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_181]\nat java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]\nCaused by: java.lang.ClassNotFoundException: org.apache.curator.connection.ConnectionHandlingPolicy atjava.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[?:1.8.0_181]\nat java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_181]\nat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) ~[?:1.8.0_181]\nat java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_181]\n... 39 more\n")],-1),n("p",null,[o("解法：原因是Curator的版本和zookeeper的版本有对应关系。Curator2.X的吧，对于Curator2.X是支持Zookeeper3.4.X的，所以如果目前你是Zookeeper3.4.X的版本，还是使用Curator2.X的吧，比如：2.7.0。参考链接："),n("a",{href:"https://blog.csdn.net/muyingmiao/article/details/100183768"},"https://blog.csdn.net/muyingmiao/article/details/100183768")],-1),n("h4",null,"Q15、python引擎调度时，报如下错误Python proces is not alive：",-1),n("p",null,[n("img",{src:$,alt:"linkis-exception-08.png"})],-1),n("p",null,"解法：服务器安装anaconda3 包管理器，经过对python调试，发现两个问题：（1）缺乏pandas、matplotlib模块，需要手动安装;(2)新版python引擎执行时，依赖python高版本，首先安装python3，其次做下符号链接（如下图），重启engineplugin服务。",-1),n("p",null,[n("img",{src:f,alt:"shell-error-02.png"})],-1),n("h4",null,"Q16、spark引擎执行时，报如下错误NoClassDefFoundError: org/apache/hadoop/hive/ql/io/orc/OrcFile：",-1),n("pre",null,[n("code",null,"2021-03-19 15:12:49.227 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler 57 logInfo -ShuffleMapStage 5 (show at <console>:69) failed in 21.269 s due to Job aborted due to stage failure: Task 1 in stage 5.0 failed 4 times, most recent failure: Lost task 1.3 in stage 5.0 (TID 139, cdh03, executor 6):java.lang.NoClassDefFoundError: org/apache/hadoop/hive/ql/io/orc/OrcFile \nat org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2.apply(OrcFileOperator.scala:75)\nat org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2.apply(OrcFileOperator.scala:73)\nat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\nat scala.collection.TraversableOnce$class.collectFirst(TraversableOnce.scala:145)\nat scala.collection.AbstractIterator.collectFirst(Iterator.scala:1334)\nat org.apache.spark.sql.hive.orc.OrcFileOperator$.getFileReader(OrcFileOperator.scala:90)\nat org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$2.apply(OrcFileOperator.scala:99)\nat org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$2.apply(OrcFileOperator.scala:99)\nat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\nat scala.collection.TraversableOnce$class.collectFirst(TraversableOnce.scala:145)\nat scala.collection.AbstractIterator.collectFirst(Iterator.scala:1334)\nat org.apache.spark.sql.hive.orc.OrcFileOperator$.readSchema(OrcFileOperator.scala:99)\nat org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2.apply(OrcFileFormat.scala:160)\nat org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2.apply(OrcFileFormat.scala:151)\nat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\nat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\nat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:126)\nat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:179)\nat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:103)\nat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(UnknownSource)\nat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:624)\nat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\nat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\nat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\nat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\nat org.apache.spark.scheduler.Task.run(Task.scala:121)\nat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\nat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\nat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\nat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.io.orc.OrcFile \nat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\nat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n... 33 more\n\n")],-1),n("p",null,"解法：cdh6.3.2集群spark引擎classpath只有/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/jars，需要新增hive-exec-2.1.1-cdh6.1.0.jar，然后重启spark。",-1),n("h4",null,"Q17、spark引擎启动时，报queue default is not exists in YARN,具体信息如下：",-1),n("p",null,[n("img",{src:j,alt:"linkis-exception-09.png"})],-1),n("p",null,"解法：1.0的linkis-resource-manager-dev-1.0.0.jar拉取队列信息时，解析json有兼容问题，官方同学优化后，重新提供新包，jar包路径：/appcom/Install/dss-linkis/linkis/lib/linkis-computation-governance/linkis-cg-linkismanager/。",-1),n("h4",null,"Q18、spark引擎启动时，报错 get the Yarn queue information excepiton.(获取Yarn队列信息异常)以及http链接异常",-1),n("p",null,"解法：yarn的地址配置迁移DB配置，需要增加如下配置：",-1),n("p",null,[n("img",{src:"/assets/db-config-02.f05b1586.png",alt:"db-config-02.png"})],-1),n("h4",null,"Q19、spark引擎调度时，首次可以执行成功，再次执行报Spark application sc has already stopped, please restart it，具体错误如下：",-1),n("p",null,[n("img",{src:"/assets/page-show-03.63498698.png",alt:"page-show-03.png"})],-1),n("p",null,"解法：背景是linkis1.0引擎的架构体系有调整，spark session 创建后，为了避免开销、提升执行效率，session是复用的。当我们第一次执行spark.scala时，我们的脚本存在spark.stop()，这个命令会导致新创建的会话被关闭，当再次执行时，会提示会话已关闭，请重启。解决办法：首先所有脚本去掉stop(),其次是执行顺序：先执行default.sql，再执行scalaspark、pythonspark即可。",-1),n("h4",null,"Q20、pythonspark调度执行，报错：initialize python executor failed ClassNotFoundException org.slf4j.impl.StaticLoggerBinder，具体如下：",-1),n("p",null,[n("img",{src:y,alt:"linkis-exception-10.png"})],-1),n("p",null,"解法：原因是spark服务端缺少 slf4j-log4j12-1.7.25.jar,copy上述jar报到/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/jars。",-1),n("h4",null,"Q21、pythonspark调度执行，报错：initialize python executor failed，submit-version error，具体如下：",-1),n("p",null,[n("img",{src:b,alt:"shell-error-03.png"})],-1),n("p",null,"解法：原因是linkis1.0 pythonSpark引擎获取spark版本代码有bug，修复如下：",-1),n("p",null,[n("img",{src:"/assets/code-fix-01.620f0486.png",alt:"code-fix-01.png"})],-1),n("h4",null,"Q22、pythonspark调度执行时，报TypeError:an integer is required(got type bytes)(单独执行拉起引擎的命令跑出的)，具体如下：",-1),n("p",null,[n("img",{src:C,alt:"shell-error-04.png"})],-1),n("p",null,"解法：原因是系统spark和python版本不兼容，python是3.8，spark是2.4.0-cdh6.3.2,spark要求python version<=3.6，降低python至3.6，注释文件/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/context.py如下几行：",-1),n("p",null,[n("img",{src:w,alt:"shell-error-05.png"})],-1),n("h4",null,"Q23、spark引擎是2.4.0+cdh6.3.2，python引擎之前因为缺少pandas、matplotlib升级的本地python到3.8，但是spark还不支持python3.8，仅支持3.6以下；",-1),n("p",null,"解法：重新安装python包管理器anaconda2,将python统一降到2.7,安装pandas、matplotlib模块，python引擎、spark引擎均可以正常调度。",-1)],I={setup:(n,{expose:o})=>(o({frontmatter:{}}),(n,o)=>(e(),a("div",T,D)))},q={class:"ctn-block reading-area"},O={class:"main-content"},L={setup(o){const l=r(localStorage.getItem("locale")||"en");return(o,r)=>(e(),a("div",q,[n("main",O,["en"===l.value?(e(),t(i(S),{key:0})):(e(),t(i(I),{key:1}))])]))}};export{L as default};
