"use strict";(self.webpackChunklinkis_web_apache=self.webpackChunklinkis_web_apache||[]).push([[10],{3905:function(e,n,o){o.d(n,{Zo:function(){return c},kt:function(){return u}});var t=o(7294);function i(e,n,o){return n in e?Object.defineProperty(e,n,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[n]=o,e}function a(e,n){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),o.push.apply(o,t)}return o}function r(e){for(var n=1;n<arguments.length;n++){var o=null!=arguments[n]?arguments[n]:{};n%2?a(Object(o),!0).forEach((function(n){i(e,n,o[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):a(Object(o)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(o,n))}))}return e}function l(e,n){if(null==e)return{};var o,t,i=function(e,n){if(null==e)return{};var o,t,i={},a=Object.keys(e);for(t=0;t<a.length;t++)o=a[t],n.indexOf(o)>=0||(i[o]=e[o]);return i}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(t=0;t<a.length;t++)o=a[t],n.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(i[o]=e[o])}return i}var s=t.createContext({}),p=function(e){var n=t.useContext(s),o=n;return e&&(o="function"==typeof e?e(n):r(r({},n),e)),o},c=function(e){var n=p(e.components);return t.createElement(s.Provider,{value:n},e.children)},d={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},m=t.forwardRef((function(e,n){var o=e.components,i=e.mdxType,a=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),m=p(o),u=i,k=m["".concat(s,".").concat(u)]||m[u]||d[u]||a;return o?t.createElement(k,r(r({ref:n},c),{},{components:o})):t.createElement(k,r({ref:n},c))}));function u(e,n){var o=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var a=o.length,r=new Array(a);r[0]=m;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l.mdxType="string"==typeof e?e:i,r[1]=l;for(var p=2;p<a;p++)r[p]=o[p];return t.createElement.apply(null,r)}return t.createElement.apply(null,o)}m.displayName="MDXCreateElement"},325:function(e,n,o){o.r(n),o.d(n,{frontMatter:function(){return l},contentTitle:function(){return s},metadata:function(){return p},toc:function(){return c},default:function(){return m}});var t=o(7462),i=o(3366),a=(o(7294),o(3905)),r=["components"],l={title:"Compile And Package",sidebar_position:1},s="Linkis Compilation Document",p={unversionedId:"development/linkis_compile_and_package",id:"version-1.0.2/development/linkis_compile_and_package",isDocsHomePage:!1,title:"Compile And Package",description:"1. Fully compile Linkis",source:"@site/versioned_docs/version-1.0.2/development/linkis_compile_and_package.md",sourceDirName:"development",slug:"/development/linkis_compile_and_package",permalink:"/docs/latest/development/linkis_compile_and_package",editUrl:"https://github.com/apache/incubator-linkis-website/edit/dev/versioned_docs/version-1.0.2/development/linkis_compile_and_package.md",tags:[],version:"1.0.2",sidebarPosition:1,frontMatter:{title:"Compile And Package",sidebar_position:1},sidebar:"version-1.0.2/tutorialSidebar",previous:{title:"Gateway Design",permalink:"/docs/latest/architecture/microservice_governance_services/gateway"},next:{title:"Linkis Debug",permalink:"/docs/latest/development/linkis_debug"}},c=[{value:"1. Fully compile Linkis",id:"1-fully-compile-linkis",children:[]},{value:"2. Compile a single module",id:"2-compile-a-single-module",children:[]},{value:"3. Build an engine",id:"3-build-an-engine",children:[]},{value:"4. How to Modify Linkis dependency versions of Hadoop, Hive, Spark",id:"4-how-to-modify-linkis-dependency-versions-of-hadoop-hive-spark",children:[]}],d={toc:c};function m(e){var n=e.components,o=(0,i.Z)(e,r);return(0,a.kt)("wrapper",(0,t.Z)({},d,o,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"linkis-compilation-document"},"Linkis Compilation Document"),(0,a.kt)("h2",{id:"1-fully-compile-linkis"},"1. Fully compile Linkis"),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Environment requirements:")," Version of JDK must be higher then ",(0,a.kt)("strong",{parentName:"p"},"JDK8"),",  ",(0,a.kt)("strong",{parentName:"p"},"Oracle/Sun")," and ",(0,a.kt)("strong",{parentName:"p"},"OpenJDK")," are both supported."),(0,a.kt)("p",null,"After getting the project code from Git, compile the project installation package using Maven."),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Notice")," : The official recommended versions for compiling Linkis are hadoop-2.7.2, hive-1.2.1, spark-2.4.3, and Scala-2.11.12."),(0,a.kt)("p",null,"If you want to compile Linkis with another version of Hadoop, Hive, Spark, please refer to: ","[How to Modify Linkis dependency of Hadoop, Hive, Spark]","(#4 How to Modify Linkis dependency versionof Hadoop, Hive, Spark)"),(0,a.kt)("p",null,"(1) ",(0,a.kt)("strong",{parentName:"p"},"If you compile it locally for the first time, you must execute the following command ")," in the source package root directory of Linkis:**"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"cd wedatasphere-linkis-x.x.x\nmvn -N  install\n")),(0,a.kt)("p",null,"(2) Execute the following command in the source package root directory of Linkis:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"cd wedatasphere-linkis-x.x.x\nmvn clean install\n")),(0,a.kt)("p",null,"(3) Get the installation package, in the project assembly->target directory:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"ls wedatasphere-linkis-x.x.x/assembly/target/wedatasphere-linkis-x.x.x-dist.tar.gz\n")),(0,a.kt)("h2",{id:"2-compile-a-single-module"},"2. Compile a single module"),(0,a.kt)("p",null,"After getting the project code from Git, use Maven to package the project installation package."),(0,a.kt)("p",null,"(1) ",(0,a.kt)("strong",{parentName:"p"},"If you use it locally for the first time, you must execute the following command")," in the source package root directory of Linkis:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"cd wedatasphere-linkis-x.x.x\nmvn -N  install\n")),(0,a.kt)("p",null,"(2) Go to the corresponding module for compilation. For example, if you want to recompile the Entrance, command as follows:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"cd wedatasphere-linkis-x.x.x/linkis-computation-governance/linkis-entrance\nmvn clean install\n")),(0,a.kt)("p",null,"(3) Get the installation package. The compiled package will be found in the ->target directory of the corresponding module:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"ls wedatasphere-linkis-x.x.x/linkis-computation-governance/linkis-entrance/target/linkis-entrance.x.x.x.jar\n")),(0,a.kt)("h2",{id:"3-build-an-engine"},"3. Build an engine"),(0,a.kt)("p",null,"Here's an example of the Spark engine that builds Linkis:"),(0,a.kt)("p",null,"(1) ",(0,a.kt)("strong",{parentName:"p"},"If you use it locally for the first time, you must execute the following command")," in the source package root directory of Linkis:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"cd wedatasphere-linkis-x.x.x\nmvn -N  install\n")),(0,a.kt)("p",null,"(2) Jump to the directory where the Spark engine is located for compilation and packaging. The command is as follows:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"cd wedatasphere-linkis-x.x.x/linkis-engineconn-plugins/engineconn-plugins/spark\nmvn clean install\n")),(0,a.kt)("p",null,"(3) Get the installation package. The compiled package will be found in the ->target directory of the corresponding module:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"ls  wedatasphere-linkis-x.x.x/linkis-engineconn-plugins/engineconn-plugins/spark/target/linkis-engineplugin-spark-x.x.x.zip\n")),(0,a.kt)("p",null,"How do I install the Spark engine separately? Please refer to ",(0,a.kt)("a",{parentName:"p",href:"/docs/latest/development/new_engine_conn"},"Linkis Engine Plug-in Installation Documentation")),(0,a.kt)("h2",{id:"4-how-to-modify-linkis-dependency-versions-of-hadoop-hive-spark"},"4. How to Modify Linkis dependency versions of Hadoop, Hive, Spark"),(0,a.kt)("p",null,"Please note: Hadoop is a big data basic service, Linkis must rely on Hadoop for compilation;\nIf you don't want to use an engine, you don't need to set the version of the engine or compile the engine plug-in."),(0,a.kt)("p",null,"Specifically, the version of Hadoop can be modified in a different way than Spark, Hive, and other computing engines, as described below:"),(0,a.kt)("h4",{id:"how-do-i-modify-the-version-of-hadoop-that-linkis-relies-on"},"How do I modify the version of Hadoop that Linkis relies on?"),(0,a.kt)("p",null,"Enter the source package root directory of Linkis, and manually modify the Hadoop version information of the pom.xml file, as follows:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"cd wedatasphere-linkis-x.x.x\nvim pom.xml\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-xml"},"<properties>\n    <hadoop.version>2.7.2</hadoop.version> \x3c!--Change version of hadoop here--\x3e\n    <scala.version>2.11.12</scala.version>\n    <jdk.compile.version>1.8</jdk.compile.version>\n </properties>\n\n")),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Please note: If your hadoop version is hadoop3, you need to modify the pom file of linkis-hadoop-common"),"\nBecause under hadoop2.8, hdfs-related classes are in the hadoop-hdfs module, but in hadoop 3.X the corresponding classes are moved to the module hadoop-hdfs-client, you need to modify this file:\npom:Linkis/linkis-commons/linkis-hadoop-common/pom.xml\nModify the dependency hadoop-hdfs to hadoop-hdfs-client:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"  <dependency>\n             <groupId>org.apache.hadoop</groupId>\n             <artifactId>hadoop-hdfs</artifactId> \x3c!-- Replace this line with <artifactId>hadoop-hdfs-client</artifactId>--\x3e\n             <version>${hadoop.version}</version>\n </dependency>\n  Modify hadoop-hdfs to:\n <dependency>\n     <groupId>org.apache.hadoop</groupId>\n     <artifactId>hadoop-hdfs-client</artifactId>\n     <version>${hadoop.version}</version>\n </dependency>\n")),(0,a.kt)("h4",{id:"how-to-modify-spark-hive-versions-that-linkis-relies-on"},"How to modify Spark, Hive versions that Linkis relies on?"),(0,a.kt)("p",null,"Here's an example of changing the version of Spark. Go to the directory where the Spark engine is located and manually modify the Spark version information of the pom.xml file as follows:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"cd wedatasphere-linkis-x.x.x/linkis-engineconn-plugins/engineconn-plugins/spark\nvim pom.xml\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-xml"},"<properties>\n    <spark.version>2.4.3</spark.version> \x3c!-- Change the Spark version number here --\x3e\n </properties>\n\n")),(0,a.kt)("p",null,"Modifying the version of another engine is similar to changing the Spark version by going to the directory where the engine is located and manually changing the engine version information in the pom.xml file."),(0,a.kt)("p",null,"Then refer to  ","[Build an engine]","(#3 Build an engine)."))}m.isMDXComponent=!0}}]);