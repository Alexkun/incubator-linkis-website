"use strict";(self.webpackChunklinkis_web_apache=self.webpackChunklinkis_web_apache||[]).push([[3446],{3905:function(e,n,t){t.d(n,{Zo:function(){return p},kt:function(){return k}});var a=t(7294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=a.createContext({}),u=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},p=function(e){var n=u(e.components);return a.createElement(l.Provider,{value:n},e.children)},c={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},d=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=u(t),k=r,g=d["".concat(l,".").concat(k)]||d[k]||c[k]||i;return t?a.createElement(g,o(o({ref:n},p),{},{components:t})):a.createElement(g,o({ref:n},p))}));function k(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var i=t.length,o=new Array(i);o[0]=d;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var u=2;u<i;u++)o[u]=t[u];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}d.displayName="MDXCreateElement"},3028:function(e,n,t){t.r(n),t.d(n,{frontMatter:function(){return s},contentTitle:function(){return l},metadata:function(){return u},toc:function(){return p},default:function(){return d}});var a=t(7462),r=t(3366),i=(t(7294),t(3905)),o=["components"],s={title:"Spark Engine Usage",sidebar_position:2},l="Spark engine usage documentation",u={unversionedId:"engine_usage/spark",id:"version-1.0.2/engine_usage/spark",isDocsHomePage:!1,title:"Spark Engine Usage",description:"This article mainly introduces the configuration, deployment and use of spark engine in Linkis1.0.",source:"@site/versioned_docs/version-1.0.2/engine_usage/spark.md",sourceDirName:"engine_usage",slug:"/engine_usage/spark",permalink:"/docs/latest/engine_usage/spark",editUrl:"https://github.com/apache/incubator-linkis-website/edit/dev/versioned_docs/version-1.0.2/engine_usage/spark.md",tags:[],version:"1.0.2",sidebarPosition:2,frontMatter:{title:"Spark Engine Usage",sidebar_position:2},sidebar:"version-1.0.2/tutorialSidebar",previous:{title:"Shell Engine Usage",permalink:"/docs/latest/engine_usage/shell"},next:{title:"Overview",permalink:"/docs/latest/api/overview"}},p=[{value:"1. Environment configuration before using Spark engine",id:"1-environment-configuration-before-using-spark-engine",children:[]},{value:"2. Configuration and deployment of Spark engine",id:"2-configuration-and-deployment-of-spark-engine",children:[{value:"2.1 Selection and compilation of spark version",id:"21-selection-and-compilation-of-spark-version",children:[]},{value:"2.2 spark engineConn deployment and loading",id:"22-spark-engineconn-deployment-and-loading",children:[]},{value:"2.3 tags of spark engine",id:"23-tags-of-spark-engine",children:[]}]},{value:"3. Use of spark engine",id:"3-use-of-spark-engine",children:[{value:"Preparation for operation, queue setting",id:"preparation-for-operation-queue-setting",children:[]},{value:"3.1 How to use Scriptis",id:"31-how-to-use-scriptis",children:[]},{value:"3.2 How to use workflow",id:"32-how-to-use-workflow",children:[]},{value:"3.3 How to use Linkis Client",id:"33-how-to-use-linkis-client",children:[]}]},{value:"4. Spark engine user settings",id:"4-spark-engine-user-settings",children:[]}],c={toc:p};function d(e){var n=e.components,s=(0,r.Z)(e,o);return(0,i.kt)("wrapper",(0,a.Z)({},c,s,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"spark-engine-usage-documentation"},"Spark engine usage documentation"),(0,i.kt)("p",null,"This article mainly introduces the configuration, deployment and use of spark engine in Linkis1.0."),(0,i.kt)("h2",{id:"1-environment-configuration-before-using-spark-engine"},"1. Environment configuration before using Spark engine"),(0,i.kt)("p",null,"If you want to use the spark engine on your server, you need to ensure that the following environment variables have been set correctly and that the user who started the engine has these environment variables."),(0,i.kt)("p",null,"It is strongly recommended that you check these environment variables of the executing user before executing spark tasks."),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Environment variable name"),(0,i.kt)("th",{parentName:"tr",align:null},"Environment variable content"),(0,i.kt)("th",{parentName:"tr",align:null},"Remarks"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"JAVA_HOME"),(0,i.kt)("td",{parentName:"tr",align:null},"JDK installation path"),(0,i.kt)("td",{parentName:"tr",align:null},"Required")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"HADOOP_HOME"),(0,i.kt)("td",{parentName:"tr",align:null},"Hadoop installation path"),(0,i.kt)("td",{parentName:"tr",align:null},"Required")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"HADOOP_CONF_DIR"),(0,i.kt)("td",{parentName:"tr",align:null},"Hadoop configuration path"),(0,i.kt)("td",{parentName:"tr",align:null},"Required")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"HIVE","_","CONF_DIR"),(0,i.kt)("td",{parentName:"tr",align:null},"Hive configuration path"),(0,i.kt)("td",{parentName:"tr",align:null},"Required")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"SPARK_HOME"),(0,i.kt)("td",{parentName:"tr",align:null},"Spark installation path"),(0,i.kt)("td",{parentName:"tr",align:null},"Required")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"SPARK_CONF_DIR"),(0,i.kt)("td",{parentName:"tr",align:null},"Spark configuration path"),(0,i.kt)("td",{parentName:"tr",align:null},"Required")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"python"),(0,i.kt)("td",{parentName:"tr",align:null},"python"),(0,i.kt)("td",{parentName:"tr",align:null},"Anaconda's python is recommended as the default python")))),(0,i.kt)("p",null,"Table 1-1 Environmental configuration list"),(0,i.kt)("h2",{id:"2-configuration-and-deployment-of-spark-engine"},"2. Configuration and deployment of Spark engine"),(0,i.kt)("h3",{id:"21-selection-and-compilation-of-spark-version"},"2.1 Selection and compilation of spark version"),(0,i.kt)("p",null,"In theory, Linkis1.0 supports all versions of spark2.x and above. Spark 2.4.3 is the default supported version. If you want to use your spark version, such as spark2.1.0, you only need to modify the version of the plug-in spark and then compile it. Specifically, you can find the linkis-engineplugin-spark module, change the \\<spark.version",">"," tag to 2.1.0, and then compile this module separately."),(0,i.kt)("h3",{id:"22-spark-engineconn-deployment-and-loading"},"2.2 spark engineConn deployment and loading"),(0,i.kt)("p",null,"If you have already compiled your spark engine plug-in has been compiled, then you need to put the new plug-in to the specified location to load, you can refer to the following article for details"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"/docs/latest/deployment/engine_conn_plugin_installation"},"EngineConnPlugin Installation")," "),(0,i.kt)("h3",{id:"23-tags-of-spark-engine"},"2.3 tags of spark engine"),(0,i.kt)("p",null,"Linkis1.0 is done through tags, so we need to insert data in our database, the way of inserting is shown below."),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/apache/incubator-linkis/wiki/EngineConnPlugin%E5%BC%95%E6%93%8E%E6%8F%92%E4%BB%B6%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3%5C#22-%E7%AE%A1%E7%90%86%E5%8F%B0configuration%E9%85%8D%E7%BD%AE%E4%BF%AE%E6%94%B9%E5%8F%AF%E9%80%89"},"https://github.com/apache/incubator-linkis/wiki/EngineConnPlugin%E5%BC%95%E6%93%8E%E6%8F%92%E4%BB%B6%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3\\#22-%E7%AE%A1%E7%90%86%E5%8F%B0configuration%E9%85%8D%E7%BD%AE%E4%BF%AE%E6%94%B9%E5%8F%AF%E9%80%89")),(0,i.kt)("h2",{id:"3-use-of-spark-engine"},"3. Use of spark engine"),(0,i.kt)("h3",{id:"preparation-for-operation-queue-setting"},"Preparation for operation, queue setting"),(0,i.kt)("p",null,"Because the execution of spark is a resource that requires a queue, the user must set up a queue that he can execute before executing."),(0,i.kt)("p",null,(0,i.kt)("img",{src:t(7117).Z})),(0,i.kt)("p",null,"Figure 3-1 Queue settings"),(0,i.kt)("h3",{id:"31-how-to-use-scriptis"},"3.1 How to use Scriptis"),(0,i.kt)("p",null,"The use of Scriptis is the simplest. You can directly enter Scriptis and create a new sql, scala or pyspark script for execution."),(0,i.kt)("p",null,"The sql method is the simplest. You can create a new sql script and write and execute it. When it is executed, the progress will be displayed. If the user does not have a spark engine at the beginning, the execution of sql will start a spark session (it may take some time here),\nAfter the SparkSession is initialized, you can start to execute sql."),(0,i.kt)("p",null,(0,i.kt)("img",{src:t(2196).Z})),(0,i.kt)("p",null,"Figure 3-2 Screenshot of the execution effect of sparksql"),(0,i.kt)("p",null,"For spark-scala tasks, we have initialized sqlContext and other variables, and users can directly use this sqlContext to execute sql."),(0,i.kt)("p",null,(0,i.kt)("img",{src:t(1298).Z})),(0,i.kt)("p",null,"Figure 3-3 Execution effect diagram of spark-scala"),(0,i.kt)("p",null,"Similarly, in the way of pyspark, we have also initialized the SparkSession, and users can directly use spark.sql to execute SQL."),(0,i.kt)("p",null,(0,i.kt)("img",{src:t(7829).Z}),"\nFigure 3-4 pyspark execution mode"),(0,i.kt)("h3",{id:"32-how-to-use-workflow"},"3.2 How to use workflow"),(0,i.kt)("p",null,"DSS workflow also has three spark nodes. You can drag in workflow nodes, such as sql, scala or pyspark nodes, and then double-click to enter and edit the code, and then execute in the form of workflow."),(0,i.kt)("p",null,(0,i.kt)("img",{src:t(1884).Z})),(0,i.kt)("p",null,"Figure 3-5 The node where the workflow executes spark"),(0,i.kt)("h3",{id:"33-how-to-use-linkis-client"},"3.3 How to use Linkis Client"),(0,i.kt)("p",null,"Linkis also provides a client method to call spark tasks, the call method is through the SDK provided by LinkisClient. We provide java and scala two ways to call, the specific usage can refer to ",(0,i.kt)("a",{parentName:"p",href:"/docs/latest/user_guide/sdk_manual"},"JAVA SDK Manual"),"."),(0,i.kt)("h2",{id:"4-spark-engine-user-settings"},"4. Spark engine user settings"),(0,i.kt)("p",null,"In addition to the above engine configuration, users can also make custom settings, such as the number of spark session executors and the memory of the executors. These parameters are for users to set their own spark parameters more freely, and other spark parameters can also be modified, such as the python version of pyspark."),(0,i.kt)("p",null,(0,i.kt)("img",{src:t(3593).Z})),(0,i.kt)("p",null,"Figure 4-1 Spark user-defined configuration management console"))}d.isMDXComponent=!0},7829:function(e,n,t){n.Z=t.p+"assets/images/pyspakr-run-39cd0bbe6c61d2fc7ad933db99c33d06.png"},7117:function(e,n,t){n.Z=t.p+"assets/images/queue-set-e97a179515f871064f97ad6a28747f0c.png"},1298:function(e,n,t){n.Z=t.p+"assets/images/scala-run-77cd49935a85082d9346d28f3ecf44e3.png"},3593:function(e,n,t){n.Z=t.p+"assets/images/spark-conf-4feb25e555269ab20d55b0b7133ec1d1.png"},2196:function(e,n,t){n.Z=t.p+"assets/images/sparksql-run-d748d4fab0548fa92a6e91f42c911466.png"},1884:function(e,n,t){n.Z=t.p+"assets/images/workflow-10d4a1090b39c00c25a2b62f1c25ca60.png"}}]);